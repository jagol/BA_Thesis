import csv
import sys
import os
import json
import argparse
from random import sample
from typing import Dict, Any, List, Tuple
sys.path.append('../pipeline/')
from utility_functions import get_config
import pdb

"""Test how much the generated taxonomy overlaps with the taxonomy
generated by the TaxoGen team."""


term_type = Tuple[int, str, int]


class Evaluator:

    def __init__(self, cmdarg, config):
        self.config = config
        self.args = cmdarg
        self.path = config['paths'][cmdarg.location][cmdarg.corpus]['path_out']
        self.path_taxonomy = os.path.join(self.path, 'concept_terms/tax_labels_sim.csv')
        self.taxogen_tax = self.load_taxogen_tax()
        self.taxonomy = self.read_taxonomy()
        self.taxonomy[0] = {'child_ids': [1, 2, 3, 4, 5], 'terms': []}
        self.rels = []  # List of relations (hypernym, hyponym)

    @staticmethod
    def load_taxogen_tax():
        """Load the taxonomy created by the taxogen paper.

        Load the top 5 topics.
        """
        with open('taxogen_tax.json', 'r', encoding='utf8') as f:
            return json.load(f)

    def read_taxonomy(self) -> Dict[int, Dict[str, Any]]:
        """Read the taxonomy from csv input file.

        Output:
            {node-id: {'child_ids': list of child-ids, 'terms': list of
                terms, each term of the form (id, name, score)}}
        """
        taxonomy = {}
        with open(self.path_taxonomy, 'r', encoding='utf8') as f:
            reader = csv.reader(f)
            for row in reader:
                node_id = int(row[0])
                child_ids = [int(idx) for idx in row[1:6]]
                terms = row[6:]
                terms = [tuple(term.split('|')) for term in terms]
                taxonomy[node_id] = {'child_ids': child_ids, 'terms': terms}
        return taxonomy

    def compute_taxogen_sim(self):
        """Compute the similarity to the taxonomy generated by taxogen.

        Divide the number of labels in both taxonomies at level 1 or 2
        by the total number of labels in the first two levels.
        """
        top_level_terms = list(self.taxogen_tax.keys())
        second_level_terms = []
        for tlt in top_level_terms:
            for slt in self.taxogen_tax[tlt]:
                second_level_terms.append(slt)
        taxogen_terms = set(top_level_terms + second_level_terms)
        taxonomy_terms = []
        for chid1 in self.taxonomy[0]['child_ids']:
            if len(self.taxonomy[chid1]['terms']) > 0:
                taxonomy_terms.append(self.taxonomy[chid1]['terms'][0][1])
            child_ids = self.taxonomy[chid1]['child_ids']
            for chid2 in child_ids:
                if chid2 in self.taxonomy:
                    if len(self.taxonomy[chid2]['terms']) > 0:
                        taxonomy_terms.append(self.taxonomy[chid2]['terms'][0][1])
        taxonomy_terms = set(taxonomy_terms)
        common = taxonomy_terms.intersection(taxogen_terms)
        similarity = len(common) / len(taxogen_terms)
        print('TaxoGen vs own implementation:')
        msg = 'total: {}, common: {}, similarity: {}'
        print(msg.format(len(taxogen_terms), len(common), similarity))

    def generate_eval_file(self):
        """Generate a csv file to evaluate the relation precision."""
        n = 10
        self.rec_get_rels(0)
        self.rels = [((1, 'sample_term', 0.5), (2, 'hyponym', 3.4))]*10
        # rels_subset = self.pick_n_random(self.rels, n)
        rels_subset = sample(self.rels, n)
        self.write_rels_to_file(rels_subset)

    def rec_get_rels(self, node_id: int) -> None:
        hyper_terms = self.taxonomy[node_id]['terms']
        hypo_terms = []
        for child_id in self.taxonomy[node_id]['child_ids']:
            if child_id not in self.taxonomy:
                return
            hypo_terms.extend(self.taxonomy[child_id]['terms'][:1])
            for hyper in hyper_terms:
                for hypo in hypo_terms:
                    self.rels.append((hyper, hypo))
            self.rec_get_rels(child_id)

    def write_rels_to_file(self,
                           rels_subset: List[Tuple[term_type, term_type]]
                           ) -> None:
        """Write the given relations to a csv-file.

        Each row has the form:
        hyper_idx,hyper_name,hyper_score,hypo_idx,hyper_name,hyper_score
        """
        # out_path = os.path.join(self.path, 'evaluation/eval_rel_prec.csv')
        with open('eval_rel_prec.csv', 'w', encoding='utf8') as f:
            writer = csv.writer(f)
            for rel in rels_subset:
                hpe_idx, hpe_name, hpe_sim = rel[0]
                hpo_idx, hpo_name, hpo_sim = rel[1]
                row = [hpe_idx, hpe_name, hpe_sim, hpo_idx, hpo_name, hpo_sim]
                writer.writerow(row)

    @staticmethod
    def compute_rel_prec():
        """Compute the relation precision for a given annotation.

        Look for a file named 'eval_rel_prec.csv' in this directory.
        Every row has the form:
        Each row has the form:
        hyper_idx,hyper_name,hyper_score,
        hypo_idx,hyper_name,hyper_score,0/1
        0/1 denotes a true or false relation.
        """
        num_true = 0
        total = 0
        with open('eval_rel_prec.csv', 'r', encoding='utf8') as f:
            reader = csv.reader(f)
            for row in reader:
                true_or_not = int(row[6])
                num_true += true_or_not
                total += 1
        num_false = total - num_true
        precision = num_true / total
        msg = 'total: {}, true: {}, false: {}, precision: {}'
        print(msg.format(total, num_true, num_false, precision))


def main():
    config = get_config()
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-ge',
        '--gen_eval',
        help='Generate evaluation files.',
    )
    parser.add_argument(
        '-cp',
        '--calc_pre',
        help='Calculate the relation precision.',
    )
    parser.add_argument(
        '-l',
        '--location',
        help='indicate if local paths or server paths should be used',
    )
    parser.add_argument(
        '-c',
        '--corpus',
        help='name of corpus to be processed: europarl; dblp; sp;'
    )
    args = parser.parse_args()
    e = Evaluator(args, config)
    e.generate_eval_file()
    e.compute_taxogen_sim()
    e.compute_rel_prec()


if __name__ == '__main__':
    main()
