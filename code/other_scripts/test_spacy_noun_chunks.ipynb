{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test noun phrase extraction in spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "sys.path.append('./code/pipeline')\n",
    "from ling_preprocessing import *\n",
    "nlp = spacy.load(\"/usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 0), ('first', 1), ('sentence', 2), ('is', 3), ('a', 4), ('pure', 5), ('test', 6), ('.', 7), ('The', 8), ('second', 9), ('is', 10), ('a', 11), ('grey', 12), ('mouse', 13), ('.', 14)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"The first sentence is a pure test. The second is a grey mouse. Water is interesting.\"\n",
    "doc2 = \"The first sentence is a pure test . The second is a grey mouse .\".split(' ')\n",
    "print([(doc2[i], i) for i in range(len(doc2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_doc = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence 0 2 : 2 sentence\n",
      "a pure test 4 6 : 2 test\n",
      "The second 8 9 : 1 second\n",
      "a grey mouse 11 13 : 2 mouse\n",
      "Water 15 15 : 0 Water\n"
     ]
    }
   ],
   "source": [
    "for np in nlp_doc.noun_chunks:\n",
    "    head_idx = np.root.i-np.start\n",
    "    print(np, np.start, np.root.i, ':', head_idx, np[head_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wihtout sentence segregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'first', 'sentence', 'is', 'a', 'pure', 'test', '.', 'The', 'second', 'is', 'a', 'grey', 'mouse', '.', 'Water', 'is', 'interesting', '.']\n",
      "[('The_first_sentence', 0, 3), ('a_pure_test', 4, 7), ('The_second', 8, 10), ('a_grey_mouse', 11, 14), ('Water', 15, 16)]\n",
      "Water\n",
      "['Water']\n",
      "a_grey_mouse\n",
      "['a', 'grey', 'mouse']\n",
      "The_second\n",
      "['The', 'second']\n",
      "a_pure_test\n",
      "['a', 'pure', 'test']\n",
      "The_first_sentence\n",
      "['The', 'first', 'sentence']\n",
      "['The_first_sentence', 'is', 'a_pure_test', '.', 'The_second', 'is', 'a_grey_mouse', '.', 'Water', 'is', 'interesting', '.']\n"
     ]
    }
   ],
   "source": [
    "nps = []\n",
    "doc_tokens = [t.text for t in nlp_doc]\n",
    "print(doc_tokens)\n",
    "for np in nlp_doc.noun_chunks:\n",
    "    np = nlp_doc[np.start: np.end]\n",
    "    np_concat = '_'.join(np.text.split(' '))\n",
    "    nps.append((np_concat, np.start, np.end))\n",
    "print(nps)\n",
    "for n in nps[::-1]:\n",
    "    print(n[0])\n",
    "    print(doc_tokens[n[1]:n[2]])\n",
    "    doc_tokens[n[1]:n[2]] = [n[0]]\n",
    "print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentence segretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Water', 15, 16)]\n",
      "------------------------------\n",
      "[('grey_mouse', 11, 14), ('second', 8, 10)]\n",
      "------------------------------\n",
      "[('pure_test', 4, 7), ('first_sentence', 0, 3)]\n",
      "------------------------------\n",
      "[['The', 'first_sentence', 'is', 'a', 'pure_test', '.'], ['The', 'second', 'is', 'a', 'grey_mouse', '.'], ['Water', 'is', 'interesting', '.']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# doc_sents_tokens = [token.text for sent in nlp_doc.sents for token in sent]\n",
    "doc_sents = []\n",
    "for sent in list(nlp_doc.sents)[::-1]:\n",
    "    s = []\n",
    "    for word in sent:\n",
    "        s.append(word.text)\n",
    "    \n",
    "    nps = []\n",
    "    for np in list(sent.noun_chunks)[::-1]:\n",
    "        np_slice = sent[np.start-sent.start: np.end-sent.start]\n",
    "        # print(np_slice, np_slice[0], type(np_slice), type(np_slice[0]))\n",
    "        add = 0\n",
    "        if np_slice[0].text in ['a', 'A', 'the', 'The']:\n",
    "            add += 1\n",
    "            np_slice = np_slice[1:]\n",
    "        np_concat = '_'.join(np_slice.text.split(' '))\n",
    "        # print(repr(np), np.start, np.end, sent.start, np.start-sent.start, np.end-sent.start)\n",
    "        s[np.start-sent.start+add: np.end-sent.start] = [np_concat]\n",
    "        # print(s)\n",
    "        nps.append((np_concat, np.start, np.end))\n",
    "    print(nps)\n",
    "    print(30*'-')\n",
    "    \n",
    "    doc_sents.append(s)\n",
    "    \n",
    "print(doc_sents[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: document 1 of 4327497\r",
      "\r",
      "[[[('The', 'DT', 'the', False), ('first_sentence', '1np', 'first_sentence', False), ('is', 'VBZ', 'be', True), ('a', 'DT', 'a', True), ('pure_test', '1np', 'pure_test', False), ('.', '.', '.', False)], [('The', 'DT', 'the', False), ('second', '0np', 'second', False), ('is', 'VBZ', 'be', True), ('a', 'DT', 'a', True), ('grey_mouse', '1np', 'grey_mouse', False), ('.', '.', '.', False)], [('Water', '0np', 'water', False), ('is', 'VBZ', 'be', True), ('interesting', 'JJ', 'interesting', False), ('.', '.', '.', False)]]]\n"
     ]
    }
   ],
   "source": [
    "pp = DBLPLingPreprocessor('.', '.', \"/usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-2.0.0\")\n",
    "pp._process_doc(doc, True)\n",
    "print(pp._pp_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
