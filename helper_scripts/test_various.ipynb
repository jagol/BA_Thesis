{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"/usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I like the opera. On sundays I go there.\"\n",
    "doc_nlp = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "I\n",
      "-PRON-\n",
      "2\n",
      "4\n",
      "the opera\n",
      "the_opera\n",
      "6\n",
      "7\n",
      "\n",
      "sunday\n",
      "7\n",
      "8\n",
      "\n",
      "-PRON-\n"
     ]
    }
   ],
   "source": [
    "for s in doc_nlp.sents:\n",
    "    nps = s.noun_chunks\n",
    "    for np in nps:\n",
    "        print(np.start)\n",
    "        print(np.end)\n",
    "        print(s[np.start: np.end])\n",
    "        cnp = re.sub(' ', '_', np.lemma_)\n",
    "        print(cnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test preparing sentence for elmo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'The arm_chair_radio of the couch is in the greek_mythology .'.split()\n",
    "sent_terms = [(1, ['arm', 'chair', 'radio'], 1), (1, ['couch'], 4), (1, ['greek', 'mythology'], 8)]\n",
    "sent1 = 'Sometimes machine_learning is difficult'.split()\n",
    "sent_terms1 = [(1, ['machine', 'learning'], 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from collections import defaultdict\n",
    "def prepare_sentence(sent: List[str],\n",
    "                     sent_terms: List[Tuple[str, List[str], int]]\n",
    "                     ) -> Tuple[List[str], List[List[int]]]:\n",
    "    \"\"\"Make sent where the words of terms are split into single words.\n",
    "\n",
    "    Args:\n",
    "        sent: The original sentence.\n",
    "        sent_terms: A list of terms in the sentence. Each term in the\n",
    "        sentence is represented by a Tuple in the form\n",
    "        (term-id, list of words in term, index of term in the sentence)\n",
    "    Return:\n",
    "        1. The sentence, but all multiword terms are split into single\n",
    "        words again.\n",
    "        2. Dict of term-indices of the form: {i: [indices_in_sent]}\n",
    "        i is the position of the term in sent_terms. \n",
    "    \"\"\"\n",
    "    for i in range(len(sent_terms))[::-1]:\n",
    "        term = sent_terms[i]\n",
    "        term_idx = term[2]\n",
    "        term_words = [t+str(i) for t in term[1]]\n",
    "        sent[term_idx] = term_words\n",
    "    \n",
    "    # Flatten list.\n",
    "    flat_sent = []\n",
    "    for w in sent:\n",
    "        if isinstance(w, list):\n",
    "            for v in w:\n",
    "                flat_sent.append(v)\n",
    "        else:\n",
    "            flat_sent.append(w)\n",
    "    \n",
    "    # Get indices and clean sent. Only works if there are less then 10 terms in one sentence.\n",
    "    prepped_sent = []\n",
    "    indices_in_sent = defaultdict(list)\n",
    "    for i in range(len(flat_sent)):\n",
    "        w = flat_sent[i]\n",
    "        if w[-1].isdigit():\n",
    "            term_idx = int(w[-1])\n",
    "            indices_in_sent[term_idx].append(i)\n",
    "            w = w[:-1]\n",
    "            prepped_sent.append(w)\n",
    "        else:\n",
    "            prepped_sent.append(w)\n",
    "    \n",
    "    return prepped_sent, indices_in_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The',\n",
       "  'arm',\n",
       "  'chair',\n",
       "  'radio',\n",
       "  'of',\n",
       "  'the',\n",
       "  'couch',\n",
       "  'is',\n",
       "  'in',\n",
       "  'the',\n",
       "  'greek',\n",
       "  'mythology',\n",
       "  '.'],\n",
       " defaultdict(list, {0: [1, 2, 3], 1: [6], 2: [10, 11]}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sentence(sent, sent_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test calculating the embedding of a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "def get_term_emb(embs: List[List[float]],\n",
    "                 term: List[int]\n",
    "                 ) -> List[float]:\n",
    "    \"\"\"Get the embedding for the given term.\n",
    "\n",
    "    For multiword terms, get the average embedding.\n",
    "    \n",
    "    Args:\n",
    "        embs: A list of word embeddings.\n",
    "        term: The indices of a term's words in the current sentence.\n",
    "    Return:\n",
    "        The embedding of the term.\n",
    "    \"\"\"\n",
    "    print(term)\n",
    "    term_embs = [embs[i] for i in term]\n",
    "    print(term_embs)\n",
    "    mean_emb = mean(term_embs, 0)\n",
    "    return [float(f) for f in mean_emb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = [[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4], [0.5, 0.5, 0.5], [0.6, 0.6, 0.6], [0.7, 0.7, 0.7], [0.8, 0.8, 0.8], [0.9, 0.9, 0.9], [0.1, 0.1, 0.1], [0.11, 0.11, 0.11], [0.12, 0.12, 0.12]]\n",
    "term = [9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10]\n",
      "[[0.1, 0.1, 0.1], [0.11, 0.11, 0.11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10500000000000001, 0.10500000000000001, 0.10500000000000001]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term_emb(embs, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([[1, 2], [2, 3]], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the merge dictionaries function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def merge_dicts(fpaths: List[str], path_out: str) -> None:\n",
    "    \"\"\"Merge n dictionaries to one dictionary and write to file.\n",
    "    \n",
    "    Args:\n",
    "        fpaths: A list of paths to json files that contain the dictionaries.\n",
    "        path_out: Path to the output json file.\n",
    "    Return:\n",
    "        Write a dictionary with the structure \n",
    "        Dict[int, Dict[int, List[float]]] to file. \n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for fp in fpaths:\n",
    "        with open(fp, 'r', encoding='utf8') as f:\n",
    "            cur_dict = json.load(f)\n",
    "            for term_id in cur_dict:\n",
    "                if term_id in out_dict:\n",
    "                    cur_term_dict = cur_dict[term_id]\n",
    "                    out_dict_term = out_dict[term_id]\n",
    "                    for doc_id in embs_term:\n",
    "                        if doc_id in out_dict_term[doc_id]:\n",
    "                            print('Two files have the same doc_ids...there is something wrong!')\n",
    "                        else:\n",
    "                            out_dict_term[doc_id] = cur_term_dict[doc_id]\n",
    "                else:\n",
    "                    out_dict[term_id] = cur_dict[term_id]\n",
    "    with open(path_out, 'w', encoding='utf8') as f:\n",
    "        json.dump(out_dict)\n",
    "    return outdict\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
